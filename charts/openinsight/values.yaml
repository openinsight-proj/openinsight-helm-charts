# Default values for openinsight-helm-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

clickhouse:
  enabled: true
  auth:
    username: default
    password: "changeme"
  extraEnvVars:
    - name: TZ
      value: "UTC"
  tls:
    enabled: false
  initdbScripts:
    my_init_script.sh: |
       #!/bin/bash
       set -e

       clickhouse client -u default --password='changeme' -n <<-EOSQL
         CREATE DATABASE openinsight;
       EOSQL

opentelemetry-collector:
  enabled: true
  nameOverride: "openinsight"
  mode: "deployment"
  image:
    # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
    repository: ghcr.m.daocloud.io/openinsight-proj/openinsight
    pullPolicy: IfNotPresent
    tag: "4bcfe66f65b9edf15786eafa13418b8b744af19c"
  presets:
    # Configures the collector to collect logs.
    # Adds the filelog receiver to the logs pipeline
    # and adds the necessary volumes and volume mounts.
    # Best used with mode = daemonset.
    logsCollection:
      enabled: false
      includeCollectorLogs: false
    # Configures the collector to collect host metrics.
    # Adds the hostmetrics receiver to the metrics pipeline
    # and adds the necessary volumes and volume mounts.
    # Best used with mode = daemonset.
    hostMetrics:
      enabled: false
    # Configures the Kubernetes Processor to add Kubernetes metadata.
    # Adds the k8sattributes processor to all the pipelines
    # and adds the necessary rules to ClusteRole.
    # Best used with mode = daemonset.
    kubernetesAttributes:
      enabled: false
    # Configures the Kubernetes Cluster Receiver to collect cluster-level metrics.
    # Adds the k8s_cluster receiver to the metrics pipeline
    # and adds the necessary rules to ClusteRole.
    # Best used with mode = deployment or statefulset.
    clusterMetrics:
      enabled: false
    # Configures the collector to collect Kubelet metrics.
    # Adds the kubeletstats receiver to the metrics pipeline
    # and adds the necessary rules to ClusteRole.
    # Best used with mode = daemonset.
    kubeletMetrics:
      enabled: false

  configMap:
    # Specifies whether a configMap should be created (true by default)
    create: true
  config:
    exporters:
      logging: {}
      prometheus:
        endpoint: "0.0.0.0:8889"
      clickhouse:
        dsn: tcp://default:changeme@my-openinsight-clickhouse-headless:9000/openinsight
        logs_table_name: openinsight_logs
        traces_table_name: openinsight_traces
        metrics_table_name: openinsight_metrics
        ttl_days: 3
        timeout: 10s
        sending_queue:
          queue_size: 100
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
    extensions:
      # The health_check extension is mandatory for this chart.
      # Without the health_check extension the collector will fail the readiness and liveliness probes.
      # The health_check extension can be modified, but should never be removed.
      health_check: {}
      memory_ballast: {}
      query:
        protocols:
          http:
            endpoint: 0.0.0.0:18888
          grpc:
            endpoint: 0.0.0.0:18889
        storage:
          clickhouse:
            dsn: tcp://default:changeme@my-openinsight-clickhouse-headless:9000/openinsight
            logging_table_name: openinsight_logs
            tracing_table_name: openinsight_traces
            metrics_table_name: openinsight_metrics
            tls:
              insecure: true
              insecure_skip_verify: true
        tracing_query:
          storage_type: clickhouse
        logging_query:
          storage_type: clickhouse
        metrics_query:
          storage_type: clickhouse
    processors:
      batch: {}
      # If set to null, will be overridden with values based on k8s resource limits
      memory_limiter: null
      spanmetrics:
        metrics_exporter: prometheus
        latency_histogram_buckets: [2ms, 4ms, 6ms, 8ms, 10ms, 50ms, 100ms, 200ms, 500ms, 800ms, 1s, 1400ms, 2s, 5s, 10s, 15s]
        aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
        dimensions_cache_size: 4000
      servicegraph:
        metrics_exporter: prometheus
        latency_histogram_buckets: [2ms, 4ms, 6ms, 8ms, 10ms, 50ms, 100ms, 200ms, 500ms, 800ms, 1s, 1400ms, 2s, 5s, 10s, 15s]
        store:
          ttl: 10s
          max_items: 100000
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
          thrift_compact:
            endpoint: 0.0.0.0:6831
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
            - job_name: opentelemetry-collector
              scrape_interval: 10s
              static_configs:
                - targets:
                    - ${MY_POD_IP}:8888
            - job_name: span-metrics
              scrape_interval: 10s
              static_configs:
                - targets:
                    - ${MY_POD_IP}:8999
      zipkin:
        endpoint: 0.0.0.0:9411
      fluentforward:
        endpoint: 0.0.0.0:8006
    service:
      telemetry:
        metrics:
          address: 0.0.0.0:8888
      extensions:
        - memory_ballast
        - query
        - health_check
      pipelines:
        logs:
          exporters:
            - logging
            - clickhouse
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
            - fluentforward
        metrics:
          exporters:
            - logging
            - clickhouse
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
            - prometheus
        traces:
          exporters:
            - logging
            - clickhouse
          processors:
            - memory_limiter
            - servicegraph
            - spanmetrics
            - batch
          receivers:
            - otlp
            - jaeger
            - zipkin
  ports:
    query-http:
      enabled: true
      containerPort: 18888
      servicePort: 18888
      hostPort: 18888
      protocol: TCP
    query-grpc:
      enabled: true
      containerPort: 18889
      servicePort: 18889
      hostPort: 18889
      protocol: TCP
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      hostPort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      hostPort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: true
      containerPort: 6831
      servicePort: 6831
      hostPort: 6831
      protocol: UDP
    jaeger-thrift:
      enabled: true
      containerPort: 14268
      servicePort: 14268
      hostPort: 14268
      protocol: TCP
    jaeger-grpc:
      enabled: true
      containerPort: 14250
      servicePort: 14250
      hostPort: 14250
      protocol: TCP
    zipkin:
      enabled: true
      containerPort: 9411
      servicePort: 9411
      hostPort: 9411
      protocol: TCP
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: false
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
    fluent-forward:
      enabled: true
      containerPort: 8006
      servicePort: 8006
      hostPort: 8006
      protocol: TCP

  # Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
  resources:
    limits:
      cpu: 1000m
      memory: 1024Mi

  service:
    type: NodePort
