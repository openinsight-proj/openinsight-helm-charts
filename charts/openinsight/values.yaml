# Default values for openinsight-helm-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

clickhouse:
  enabled: true
  zookeeper:
    enabled: true
    replicaCount: 5
  shards: 5
  replicaCount: 1
  resources:
    requests:
      cpu: 1000m
      memory: 1024Mi
    limits:
      cpu: 2000m
      memory: 2048Mi
  auth:
    username: default
    password: "changeme"
  extraEnvVars:
    - name: TZ
      value: "UTC"
  tls:
    enabled: false
  initdbScripts:
    my_init_script.sh: |
       #!/bin/bash
       set -e

       clickhouse client -u default --password='changeme' -n <<-EOSQL
         CREATE DATABASE openinsight;
       EOSQL
  defaultConfigurationOverrides: |
    <clickhouse>
      <!-- Macros -->
      <macros>
        <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
        <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
        <layer>{{ include "common.names.fullname" . }}</layer>
      </macros>
      <!-- Log Level -->
      <logger>
        <level>{{ .Values.logLevel }}</level>
      </logger>
      {{- if or (ne (int .Values.shards) 1) (ne (int .Values.replicaCount) 1)}}
      <!-- Cluster configuration - Any update of the shards and replicas requires helm upgrade -->
      <remote_servers>
        <secret>foo</secret>
        <default>
          {{- $shards := $.Values.shards | int }}
          {{- range $shard, $e := until $shards }}
          <shard>
              {{- $replicas := $.Values.replicaCount | int }}
              {{- range $i, $_e := until $replicas }}
              <replica>
                  <host>{{ printf "%s-shard%d-%d.%s.%s.svc.%s" (include "common.names.fullname" $ ) $shard $i (include "clickhouse.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
                  <port>{{ $.Values.service.ports.tcp }}</port>
              </replica>
              {{- end }}
          </shard>
          {{- end }}
        </default>
      </remote_servers>
      {{- end }}
      {{- if or .Values.zookeeper.enabled .Values.externalZookeeper.servers }}
      <!-- Zookeeper configuration -->
      <zookeeper>
        {{- if .Values.zookeeper.enabled }}
        {{/* Zookeeper configuration using the helm chart */}}
        {{- $nodes := .Values.zookeeper.replicaCount | int }}
        {{- range $node, $e := until $nodes }}
        <node>
          <host>{{ printf "%s-%d.%s.%s.svc.%s" (include "clickhouse.zookeeper.fullname" $ ) $node (include "clickhouse.zookeeper.headlessServiceName" $) (include "common.names.namespace" $) $.Values.clusterDomain }}</host>
          <port>{{ $.Values.zookeeper.service.ports.client }}</port>
        </node>
        {{- end }}
        {{- else if .Values.externalZookeeper.servers }}
        {{/* Zookeeper configuration using an external instance */}}
        {{- range $node :=.Values.externalZookeeper.servers }}
        <node>
          <host>{{ $node }}</host>
          <port>{{ $.Values.externalZookeeper.port }}</port>
        </node>
        {{- end }}
        {{- end }}
      </zookeeper>
      {{- end }}
      {{- if .Values.tls.enabled }}
      <!-- TLS configuration -->
      <tcp_port_secure from_env="CLICKHOUSE_TCP_SECURE_PORT"></tcp_port_secure>
      <openSSL>
          <server>
              {{- $certFileName := default "tls.crt" .Values.tls.certFilename }}
              {{- $keyFileName := default "tls.key" .Values.tls.certKeyFilename }}
              <certificateFile>/bitnami/clickhouse/certs/{{$certFileName}}</certificateFile>
              <privateKeyFile>/bitnami/clickhouse/certs/{{$keyFileName}}</privateKeyFile>
              <verificationMode>none</verificationMode>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              {{- if or .Values.tls.autoGenerated .Values.tls.certCAFilename }}
              {{- $caFileName := default "ca.crt" .Values.tls.certFilename }}
              <caConfig>/bitnami/clickhouse/certs/{{$caFileName}}</caConfig>
              {{- else }}
              <loadDefaultCAFile>true</loadDefaultCAFile>
              {{- end }}
          </server>
          <client>
              <loadDefaultCAFile>true</loadDefaultCAFile>
              <cacheSessions>true</cacheSessions>
              <disableProtocols>sslv2,sslv3</disableProtocols>
              <preferServerCiphers>true</preferServerCiphers>
              <verificationMode>none</verificationMode>
              <invalidCertificateHandler>
                  <name>AcceptCertificateHandler</name>
              </invalidCertificateHandler>
          </client>
      </openSSL>
      {{- end }}
      {{- if .Values.metrics.enabled }}
       <!-- Prometheus metrics -->
       <prometheus>
          <endpoint>/metrics</endpoint>
          <port from_env="CLICKHOUSE_METRICS_PORT"></port>
          <metrics>true</metrics>
          <events>true</events>
          <asynchronous_metrics>true</asynchronous_metrics>
      </prometheus>
      {{- end }}
    </clickhouse>
opentelemetry-collector:
  enabled: true
  nameOverride: "openinsight"
  mode: "deployment"
  image:
    # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
    repository: ghcr.m.daocloud.io/openinsight-proj/openinsight
    pullPolicy: IfNotPresent
    tag: "a82cadbf0363bcce0ef7adc4da34ab245f3a299e"
  presets:
    # Configures the collector to collect logs.
    # Adds the filelog receiver to the logs pipeline
    # and adds the necessary volumes and volume mounts.
    # Best used with mode = daemonset.
    logsCollection:
      enabled: false
      includeCollectorLogs: false
    # Configures the collector to collect host metrics.
    # Adds the hostmetrics receiver to the metrics pipeline
    # and adds the necessary volumes and volume mounts.
    # Best used with mode = daemonset.
    hostMetrics:
      enabled: false
    # Configures the Kubernetes Processor to add Kubernetes metadata.
    # Adds the k8sattributes processor to all the pipelines
    # and adds the necessary rules to ClusteRole.
    # Best used with mode = daemonset.
    kubernetesAttributes:
      enabled: false
    # Configures the Kubernetes Cluster Receiver to collect cluster-level metrics.
    # Adds the k8s_cluster receiver to the metrics pipeline
    # and adds the necessary rules to ClusteRole.
    # Best used with mode = deployment or statefulset.
    clusterMetrics:
      enabled: false
    # Configures the collector to collect Kubelet metrics.
    # Adds the kubeletstats receiver to the metrics pipeline
    # and adds the necessary rules to ClusteRole.
    # Best used with mode = daemonset.
    kubeletMetrics:
      enabled: false

  configMap:
    # Specifies whether a configMap should be created (true by default)
    create: true
  config:
    exporters:
      logging: {}
      prometheus:
        endpoint: "0.0.0.0:8889"
      clickhouse:
        dsn: tcp://default:changeme@{{ .Release.Name }}-clickhouse-headless:9000/openinsight
        logs_table_name: openinsight_logs
        traces_table_name: openinsight_traces
        metrics_table_name: openinsight_metrics
        ttl_days: 3
        timeout: 10s
        sending_queue:
          queue_size: 100
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
    extensions:
      # The health_check extension is mandatory for this chart.
      # Without the health_check extension the collector will fail the readiness and liveliness probes.
      # The health_check extension can be modified, but should never be removed.
      health_check: {}
      memory_ballast: {}
      query:
        protocols:
          http:
            endpoint: 0.0.0.0:18888
          grpc:
            endpoint: 0.0.0.0:18889
        storage:
          clickhouse:
            dsn: tcp://default:changeme@{{ .Release.Name }}-clickhouse-headless:9000/openinsight
            logging_table_name: openinsight_logs
            tracing_table_name: openinsight_traces
            metrics_table_name: openinsight_metrics
            tls:
              insecure: true
              insecure_skip_verify: true
        tracing_query:
          storage_type: clickhouse
        logging_query:
          storage_type: clickhouse
        metrics_query:
          storage_type: clickhouse
    processors:
      batch: {}
      # If set to null, will be overridden with values based on k8s resource limits
      memory_limiter: null
      spanmetrics:
        metrics_exporter: prometheus
        latency_histogram_buckets: [2ms, 4ms, 6ms, 8ms, 10ms, 50ms, 100ms, 200ms, 500ms, 800ms, 1s, 1400ms, 2s, 5s, 10s, 15s]
        aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
        dimensions_cache_size: 4000
      servicegraph:
        metrics_exporter: prometheus
        latency_histogram_buckets: [2ms, 4ms, 6ms, 8ms, 10ms, 50ms, 100ms, 200ms, 500ms, 800ms, 1s, 1400ms, 2s, 5s, 10s, 15s]
        store:
          ttl: 10s
          max_items: 100000
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
          thrift_compact:
            endpoint: 0.0.0.0:6831
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
            - job_name: opentelemetry-collector
              scrape_interval: 5s
              static_configs:
                - targets:
                    - ${MY_POD_IP}:8888
            - job_name: span-metrics
              scrape_interval: 5s
              static_configs:
                - targets:
                    - ${MY_POD_IP}:8999
      # Dummy receiver that's never used, because a pipeline is required to have one.
      otlp/otel_metrics:
        protocols:
          grpc:
            endpoint: "localhost:65535"
      zipkin:
        endpoint: 0.0.0.0:9411
      fluentforward:
        endpoint: 0.0.0.0:8006
    service:
      telemetry:
        metrics:
          address: 0.0.0.0:8888
      extensions:
        - memory_ballast
        - query
        - health_check
      pipelines:
        logs:
          exporters:
            - logging
            - clickhouse
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
            - fluentforward
        metrics/span_metrics:
          exporters:
            - prometheus
          receivers:
            - otlp/otel_metrics
        metrics:
          exporters:
            - logging
            - clickhouse
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
            - prometheus
        traces:
          exporters:
            - logging
            - clickhouse
          processors:
            - memory_limiter
            - servicegraph
            - spanmetrics
            - batch
          receivers:
            - otlp
            - jaeger
            - zipkin
  ports:
    query-http:
      enabled: true
      containerPort: 18888
      servicePort: 18888
      hostPort: 18888
      protocol: TCP
    query-grpc:
      enabled: true
      containerPort: 18889
      servicePort: 18889
      hostPort: 18889
      protocol: TCP
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      hostPort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      hostPort: 4318
      protocol: TCP
    jaeger-compact:
      enabled: true
      containerPort: 6831
      servicePort: 6831
      hostPort: 6831
      protocol: UDP
    jaeger-thrift:
      enabled: true
      containerPort: 14268
      servicePort: 14268
      hostPort: 14268
      protocol: TCP
    jaeger-grpc:
      enabled: true
      containerPort: 14250
      servicePort: 14250
      hostPort: 14250
      protocol: TCP
    zipkin:
      enabled: true
      containerPort: 9411
      servicePort: 9411
      hostPort: 9411
      protocol: TCP
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: false
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
    fluent-forward:
      enabled: true
      containerPort: 8006
      servicePort: 8006
      hostPort: 8006
      protocol: TCP

  # Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
  resources:
    limits:
      cpu: 1000m
      memory: 1024Mi

  service:
    type: NodePort
